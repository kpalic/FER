import os
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, models
from PIL import Image
from tqdm import tqdm

# Postavke za reproducibilnost
random_seed = 42
torch.manual_seed(random_seed)

<torch._C.Generator at 0x7b6204e6f190>

# Najbolji parametri
best_learning_rate = 0.0001
best_optimizer = 'Adam'
best_batch_size = 32
best_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
num_epochs = 50  # Povećan broj epoha kako bi Early Stopping imao smisla
patience = 5  # Broj epoha bez poboljšanja prije zaustavljanja

# Definiramo prilagođeni dataset
class CustomImageDataset(Dataset):
    def __init__(self, image_dir, transform=None, exclude_files=None):
        self.image_dir = image_dir
        self.transform = transform
        self.exclude_files = exclude_files if exclude_files is not None else []
        
        self.image_paths = [os.path.join(image_dir, fname) for fname in os.listdir(image_dir) if fname not in self.exclude_files]
        self.labels = [fname.split('_')[1].split('.')[0] for fname in os.listdir(image_dir) if fname not in self.exclude_files]
        self.label_to_idx = {label: idx for idx, label in enumerate(set(self.labels))}
        self.idx_to_label = {idx: label for label, idx in self.label_to_idx.items()}

    def __len__(self):
        return len(self.image_paths)

    def __getitem__(self, idx):
        img_path = self.image_paths[idx]
        image = Image.open(img_path).convert("RGB")
        label = self.labels[idx]
        label_idx = self.label_to_idx[label]

        if self.transform:
            image = self.transform(image)

        return image, label_idx

# EarlyStopping klasa
class EarlyStopping:
    def __init__(self, patience=7, verbose=False, delta=0):
        self.patience = patience
        self.verbose = verbose
        self.counter = 0
        self.best_score = None
        self.early_stop = False
        self.val_loss_min = float('inf')
        self.delta = delta

    def __call__(self, val_loss, model):
        score = -val_loss

        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
        elif score < self.best_score + self.delta:
            self.counter += 1
            if self.verbose:
                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.save_checkpoint(val_loss, model)
            self.counter = 0

    def save_checkpoint(self, val_loss, model):
        '''Spremanje modela kada validation loss smanjen.'''
        if self.verbose:
            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')
        torch.save(model.state_dict(), 'checkpoint.pt')
        self.val_loss_min = val_loss

# Funkcija za treniranje modela
def train_model(model, criterion, optimizer, scheduler, dataloaders, dataset_sizes, device, num_epochs, early_stopping):
    for epoch in range(num_epochs):
        print(f'Epoch {epoch}/{num_epochs - 1}')
        print('-' * 10)

        # Svaka epoha ima trening i validacijski dio
        for phase in ['train', 'val']:
            if phase == 'train':
                model.train()  # Postavi model u trening mod
            else:
                model.eval()   # Postavi model u evaluacijski mod

            running_loss = 0.0
            running_corrects = 0

            # Iteriramo kroz podatke
            for inputs, labels in tqdm(dataloaders[phase], desc=f'{phase} epoch {epoch + 1}/{num_epochs}'):
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Nuliramo gradijente
                optimizer.zero_grad()

                # Forward
                with torch.set_grad_enabled(phase == 'train'):
                    outputs = model(inputs)
                    _, preds = torch.max(outputs, 1)
                    loss = criterion(outputs, labels)

                    # Backward + optimizacija samo u trening fazi
                    if phase == 'train':
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

            if phase == 'train':
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')

            # Early stopping provjera samo na validation skupu
            if phase == 'val':
                early_stopping(epoch_loss, model)

        if early_stopping.early_stop:
            print("Early stopping")
            break

    # Vraćamo najbolji model
    model.load_state_dict(torch.load('checkpoint.pt'))
    return model

# Funkcija za evaluaciju modela
def evaluate_model(model, dataloader, criterion, device):
    model.eval()
    running_loss = 0.0
    running_corrects = 0
    with torch.no_grad():
        for inputs, labels in tqdm(dataloader, desc='Evaluating'):
            inputs = inputs.to(device)
            labels = labels.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)
    total_loss = running_loss / len(dataloader.dataset)
    total_acc = running_corrects.double() / len(dataloader.dataset)
    return total_loss, total_acc

import multiprocessing
multiprocessing.freeze_support()

# Definiramo putanje do podataka
base_dir = '/kaggle/input/visual-place-recognition/data'
# data_dir = os.path.abspath(os.path.join(base_dir, '..'))
train_dir = os.path.join(base_dir, 'training')
val_dir = os.path.join(base_dir, 'validation')
test_dir = os.path.join(base_dir, 'testing')

# Specify the file to remove
file_to_remove = '55007_.DS'

# Define the dataset and dataloaders
train_dataset = CustomImageDataset(train_dir, transform=best_transform, exclude_files=[file_to_remove])
val_dataset = CustomImageDataset(val_dir, transform=best_transform)
test_dataset = CustomImageDataset(test_dir, transform=best_transform)

dataloaders = {
    'train': DataLoader(train_dataset, batch_size=best_batch_size, shuffle=True, num_workers=4, pin_memory=True),
    'val': DataLoader(val_dataset, batch_size=best_batch_size, shuffle=True, num_workers=4, pin_memory=True),
    'test': DataLoader(test_dataset, batch_size=best_batch_size, shuffle=True, num_workers=4, pin_memory=True)
}

dataset_sizes = {x: len(dataloaders[x].dataset) for x in ['train', 'val', 'test']}
class_names = train_dataset.label_to_idx.keys()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# Učitavamo predtreniran ResNet model
model_ft = models.resnet18(pretrained=True)
num_ftrs = model_ft.fc.in_features
model_ft.fc = nn.Linear(num_ftrs, len(class_names))
model_ft = model_ft.to(device)

# Use DataParallel to use both GPUs
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model_ft = torch.nn.DataParallel(model_ft, device_ids = [0,1]).to(device)

criterion = nn.CrossEntropyLoss()

if best_optimizer == 'SGD':
    optimizer_ft = optim.SGD(model_ft.parameters(), lr=best_learning_rate, momentum=0.9)
elif best_optimizer == 'Adam':
    optimizer_ft = optim.Adam(model_ft.parameters(), lr=best_learning_rate)

scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)

early_stopping = EarlyStopping(patience=patience, verbose=True)

print(f"Training model with learning_rate={best_learning_rate}, optimizer={best_optimizer}, batch_size={best_batch_size}, transform={best_transform}")
model_ft = train_model(model_ft, criterion, optimizer_ft, scheduler, dataloaders, dataset_sizes, device, num_epochs=num_epochs, early_stopping=early_stopping)

# Evaluiramo model na trening, validacijskom i test skupu
train_loss, train_acc = evaluate_model(model_ft, dataloaders['train'], criterion, device)
val_loss, val_acc = evaluate_model(model_ft, dataloaders['val'], criterion, device)
test_loss, test_acc = evaluate_model(model_ft, dataloaders['test'], criterion, device)

print(f"Training Loss: {train_loss:.4f} Training Accuracy: {train_acc:.4f}")
print(f"Validation Loss: {val_loss:.4f} Validation Accuracy: {val_acc:.4f}")
print(f"Test Loss: {test_loss:.4f} Test Accuracy: {test_acc:.4f}")

# Spremamo finalni model
torch.save(model_ft.state_dict(), 'best_model.pth')

/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
100%|██████████| 44.7M/44.7M [00:00<00:00, 155MB/s]

Training model with learning_rate=0.0001, optimizer=Adam, batch_size=32, transform=Compose(
    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)
    RandomHorizontalFlip(p=0.5)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
Epoch 0/49
----------

train epoch 1/50: 100%|██████████| 11583/11583 [23:52<00:00,  8.08it/s]

train Loss: 0.8408 Acc: 0.7269

val epoch 1/50: 100%|██████████| 2483/2483 [04:58<00:00,  8.33it/s]

val Loss: 0.5296 Acc: 0.8212
Validation loss decreased (inf --> 0.529615).  Saving model ...
Epoch 1/49
----------

train epoch 2/50: 100%|██████████| 11583/11583 [20:28<00:00,  9.43it/s]

train Loss: 0.5011 Acc: 0.8315

val epoch 2/50: 100%|██████████| 2483/2483 [03:49<00:00, 10.83it/s]

val Loss: 0.4538 Acc: 0.8476
Validation loss decreased (0.529615 --> 0.453786).  Saving model ...
Epoch 2/49
----------

train epoch 3/50: 100%|██████████| 11583/11583 [20:45<00:00,  9.30it/s]

train Loss: 0.3851 Acc: 0.8692

val epoch 3/50: 100%|██████████| 2483/2483 [03:49<00:00, 10.81it/s]

val Loss: 0.3934 Acc: 0.8672
Validation loss decreased (0.453786 --> 0.393357).  Saving model ...
Epoch 3/49
----------

train epoch 4/50: 100%|██████████| 11583/11583 [20:47<00:00,  9.28it/s]

train Loss: 0.3105 Acc: 0.8941

val epoch 4/50: 100%|██████████| 2483/2483 [03:51<00:00, 10.72it/s]

val Loss: 0.3763 Acc: 0.8714
Validation loss decreased (0.393357 --> 0.376320).  Saving model ...
Epoch 4/49
----------

train epoch 5/50: 100%|██████████| 11583/11583 [20:53<00:00,  9.24it/s]

train Loss: 0.2554 Acc: 0.9127

val epoch 5/50: 100%|██████████| 2483/2483 [03:50<00:00, 10.78it/s]

val Loss: 0.3637 Acc: 0.8793
Validation loss decreased (0.376320 --> 0.363739).  Saving model ...
Epoch 5/49
----------

train epoch 6/50: 100%|██████████| 11583/11583 [20:31<00:00,  9.41it/s]

train Loss: 0.2173 Acc: 0.9254

val epoch 6/50: 100%|██████████| 2483/2483 [03:48<00:00, 10.87it/s]

val Loss: 0.3555 Acc: 0.8857
Validation loss decreased (0.363739 --> 0.355526).  Saving model ...
Epoch 6/49
----------

train epoch 7/50: 100%|██████████| 11583/11583 [20:43<00:00,  9.31it/s]

train Loss: 0.1853 Acc: 0.9364

val epoch 7/50: 100%|██████████| 2483/2483 [03:52<00:00, 10.67it/s]

val Loss: 0.3629 Acc: 0.8835
EarlyStopping counter: 1 out of 5
Epoch 7/49
----------

train epoch 8/50: 100%|██████████| 11583/11583 [20:42<00:00,  9.32it/s]

train Loss: 0.0875 Acc: 0.9718

val epoch 8/50: 100%|██████████| 2483/2483 [03:54<00:00, 10.57it/s]

val Loss: 0.2693 Acc: 0.9145
Validation loss decreased (0.355526 --> 0.269297).  Saving model ...
Epoch 8/49
----------

train epoch 9/50: 100%|██████████| 11583/11583 [20:54<00:00,  9.24it/s]

train Loss: 0.0601 Acc: 0.9814

val epoch 9/50: 100%|██████████| 2483/2483 [03:50<00:00, 10.78it/s]

val Loss: 0.2741 Acc: 0.9158
EarlyStopping counter: 1 out of 5
Epoch 9/49
----------

train epoch 10/50: 100%|██████████| 11583/11583 [20:48<00:00,  9.28it/s]

train Loss: 0.0481 Acc: 0.9854

val epoch 10/50: 100%|██████████| 2483/2483 [03:50<00:00, 10.78it/s]

val Loss: 0.2778 Acc: 0.9163
EarlyStopping counter: 2 out of 5
Epoch 10/49
----------

train epoch 11/50: 100%|██████████| 11583/11583 [20:48<00:00,  9.27it/s]

train Loss: 0.0398 Acc: 0.9881

val epoch 11/50: 100%|██████████| 2483/2483 [03:49<00:00, 10.80it/s]

val Loss: 0.2804 Acc: 0.9178
EarlyStopping counter: 3 out of 5
Epoch 11/49
----------

train epoch 12/50: 100%|██████████| 11583/11583 [20:33<00:00,  9.39it/s]

train Loss: 0.0336 Acc: 0.9902

val epoch 12/50: 100%|██████████| 2483/2483 [03:48<00:00, 10.87it/s]

val Loss: 0.2861 Acc: 0.9181
EarlyStopping counter: 4 out of 5
Epoch 12/49
----------

train epoch 13/50: 100%|██████████| 11583/11583 [20:46<00:00,  9.29it/s]

train Loss: 0.0285 Acc: 0.9918

val epoch 13/50: 100%|██████████| 2483/2483 [03:49<00:00, 10.84it/s]

val Loss: 0.2940 Acc: 0.9180
EarlyStopping counter: 5 out of 5
Early stopping

Evaluating: 100%|██████████| 11583/11583 [17:59<00:00, 10.73it/s]
Evaluating: 100%|██████████| 2483/2483 [03:53<00:00, 10.64it/s]
Evaluating: 100%|██████████| 2483/2483 [05:18<00:00,  7.80it/s]

Training Loss: 0.0308 Training Accuracy: 0.9920
Validation Loss: 0.2678 Validation Accuracy: 0.9149
Test Loss: 0.2693 Test Accuracy: 0.9153

