{"cells":[{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import cross_val_score, StratifiedKFold\n","from sklearn.metrics import make_scorer, f1_score\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import train_test_split\n","from xgboost import XGBClassifier"]},{"cell_type":"markdown","metadata":{},"source":["1. Cross-validation function\n","\n","Write a function that receives data as input, does a 5-fold cross-validation and tests the following models (with\n","default hyper-parameters): **GaussianNB, LogisticRegression, RandomForestClassifier, ExtraTreesClassifier, and\n","XGBClassifier.**\n","\n","The xgboost library is used for the XGBClassifier algorithm.\n","\n","Other algorithms are located within the sklearn library.\n","\n","The function has no return value but prints the table in the format shown below with the usage of f1_score as an\n","evaluation metric."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["def cross_validate_models(data_train, data_test, features, submission_number):\n","    # Filter the training and testing data to include only the specified features plus 'Target'\n","    X_train = data_train[features]\n","    y_train = data_train['Target']\n","    X_test = data_test[features]\n","\n","    # Define models\n","    models = {\n","        \"GaussianNB\": GaussianNB(),\n","        \"LogisticRegression\": LogisticRegression(max_iter=1000),\n","        \"RandomForestClassifier\": RandomForestClassifier(),\n","        \"ExtraTreesClassifier\": ExtraTreesClassifier(),\n","        \"XGBClassifier\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n","    }\n","\n","    # Define scoring method and folds\n","    skf = StratifiedKFold(n_splits=5)\n","    f1_scorer = make_scorer(f1_score)\n","\n","    # Evaluate each model\n","    results_display = pd.DataFrame()\n","    for name, model in models.items():\n","        cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring=f1_scorer)\n","        results_display[name] = np.append(cv_scores, cv_scores.mean())\n","\n","    results_display.index = [f'Fold {i+1}' for i in range(5)] + ['Average']\n","    print(results_display.T)\n","\n","    # Select the best model based on average F1 score\n","    best_model_name = results_display.mean(axis=0).idxmax()\n","    best_model = models[best_model_name]\n","\n","    # Retrain the best model on the entire dataset\n","    best_model.fit(X_train, y_train)\n","\n","    # Predict on the test dataset\n","    predictions = best_model.predict(X_test)\n","\n","    # Create submission DataFrame\n","    submission = data_test.loc[:, ['Id']]\n","    submission['Target'] = predictions\n","\n","    # Save submission to CSV\n","    filename = f\"submission{submission_number}.csv\"\n","    submission.to_csv(filename, index=False)\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["                          Fold 1    Fold 2    Fold 3    Fold 4    Fold 5  \\\n","GaussianNB              0.846106  0.877381  0.877484  0.877405  0.877482   \n","LogisticRegression      0.877473  0.877478  0.877478  0.877478  0.877478   \n","RandomForestClassifier  0.857616  0.864266  0.862423  0.862678  0.864373   \n","ExtraTreesClassifier    0.858159  0.866049  0.864665  0.866600  0.866736   \n","XGBClassifier           0.873122  0.876640  0.876431  0.877125  0.876989   \n","\n","                         Average  \n","GaussianNB              0.871171  \n","LogisticRegression      0.877477  \n","RandomForestClassifier  0.862271  \n","ExtraTreesClassifier    0.864442  \n","XGBClassifier           0.876062  \n"]}],"source":["data_train = pd.read_csv(\"train_clean.csv\")\n","data_test = pd.read_csv(\"test_clean.csv\")\n","features = ['Close','High','Low','Open','Volume'] # Adj close is redundant with Close\n","\n","cross_validate_models(data_train, data_test, features, 1)"]},{"cell_type":"markdown","metadata":{},"source":["**https://www.investopedia.com/top-7-technical-analysis-tools-4773275**"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Domain features\n","\n","#1 - On-Balance Volume (OBV)\n","\n","def calculate_obv(data):\n","    data = data.sort_values(by=['Symbol', 'Date'])\n","    data['OBV'] = 0  # Ensure OBV is initialized as an integer if that's the intended type\n","    for symbol in data['Symbol'].unique():\n","        symbol_data = data[data['Symbol'] == symbol].copy()\n","        price_changes = symbol_data['Close'].diff()\n","        volumes = symbol_data['Volume']\n","        obv_values = volumes.where(price_changes > 0, -volumes)\n","        obv_values.iloc[0] = 0  # Proper initialization\n","        symbol_data['OBV'] = obv_values.cumsum()\n","        symbol_data['OBV'] = symbol_data['OBV'].astype('int64')  # Explicit type casting\n","        data.loc[symbol_data.index, 'OBV'] = symbol_data['OBV']\n","        \n","    return data\n","\n","#2 - Accumulation/Distribution line\n","\n","def calculate_ad_line(data):\n","    # Calculate Money Flow Multiplier\n","    data['Money Flow Multiplier'] = ((data['Close'] - data['Low']) - (data['High'] - data['Close'])) / (data['High'] - data['Low'])\n","    # Calculate Money Flow Volume\n","    data['Money Flow Volume'] = data['Money Flow Multiplier'] * data['Volume']\n","    # Initialize AD Line in the DataFrame\n","    data['AD Line'] = 0\n","    \n","    # Calculate the AD Line without retaining intermediate columns\n","    data['AD Line'] = data['Money Flow Volume'].cumsum()\n","\n","    # Drop intermediate columns to clean up DataFrame\n","    data.drop(['Money Flow Multiplier', 'Money Flow Volume'], axis=1, inplace=True)\n","\n","    return data\n","\n","#3 - Average Directional Index (ADX)\n","\n","def calculate_adx(data, n=14):\n","    # Calculate the differences needed\n","    high_diff = data['High'].diff()\n","    low_diff = data['Low'].diff()\n","\n","    # Calculate +DM and -DM\n","    data['+DM'] = np.where((high_diff > low_diff) & (high_diff > 0), high_diff, 0.0)\n","    data['-DM'] = np.where((low_diff > high_diff) & (low_diff > 0), low_diff, 0.0)\n","\n","    # Calculate the True Range (TR)\n","    data['TR'] = np.maximum.reduce([data['High'] - data['Low'], \n","                                    abs(data['High'] - data['Close'].shift()), \n","                                    abs(data['Low'] - data['Close'].shift())])\n","\n","    # Smooth the True Range and the Directional Movements\n","    data['TR14'] = data['TR'].rolling(window=n).mean()\n","    data['+DM14'] = data['+DM'].rolling(window=n).mean()\n","    data['-DM14'] = data['-DM'].rolling(window=n).mean()\n","\n","    # Calculate Directional Indexes\n","    data['+DI14'] = 100 * data['+DM14'] / data['TR14']\n","    data['-DI14'] = 100 * data['-DM14'] / data['TR14']\n","\n","    # Calculate the ADX\n","    data['DX'] = 100 * abs(data['+DI14'] - data['-DI14']) / (data['+DI14'] + data['-DI14'])\n","    data['ADX'] = data['DX'].rolling(window=n).mean()\n","\n","    return data\n","\n","#4 - Aroon Indicator (AROON)\n","\n","def calculate_aroon(data, period=25):\n","    # Calculate Aroon Up and Aroon Down\n","    aroon_up = 100 * data['High'].rolling(window=period, min_periods=0).apply(\n","        lambda x: float(period - x.argmax()) / period, raw=True)\n","    aroon_down = 100 * data['Low'].rolling(window=period, min_periods=0).apply(\n","        lambda x: float(period - x.argmin()) / period, raw=True)\n","\n","    # Append the indicators to the DataFrame\n","    data['Aroon Up'] = aroon_up\n","    data['Aroon Down'] = aroon_down\n","\n","    return data\n","\n","#5 - MACD\n","\n","def calculate_macd(data):\n","    # Calculate the MACD and Signal Line indicators\n","    # Short term exponential moving average (EMA)\n","    ShortEMA = data.Close.ewm(span=12, adjust=False).mean() # 12-period EMA\n","    # Long term exponential moving average (EMA)\n","    LongEMA = data.Close.ewm(span=26, adjust=False).mean() # 26-period EMA\n","    # Calculate the MACD line\n","    data['MACD'] = ShortEMA - LongEMA\n","    # Calculate the signal line\n","    data['Signal_Line'] = data.MACD.ewm(span=9, adjust=False).mean()\n","    # Calculate the MACD Histogram\n","    data['MACD_Histogram'] = data['MACD'] - data['Signal_Line']\n","    \n","    return data\n","\n","#6 - Relative Strength Index (RSI)\n","\n","def calculate_rsi(data, periods=14):\n","    delta = data['Close'].diff()\n","    gain = (delta.where(delta > 0, 0)).fillna(0)\n","    loss = (-delta.where(delta < 0, 0)).fillna(0)\n","\n","    # Use the exponential moving average\n","    avg_gain = gain.ewm(span=periods, adjust=False).mean()\n","    avg_loss = loss.ewm(span=periods, adjust=False).mean()\n","\n","    rs = avg_gain / avg_loss\n","    rsi = 100 - (100 / (1 + rs))\n","\n","    data['RSI'] = rsi.fillna(0)  # Initializing RSI values to 0 before enough data\n","    return data\n","\n","\n","#7 - Stochastic Oscillator\n","\n","def calculate_stochastic_oscillator(data, periods=14):\n","    low_min = data['Low'].rolling(window=periods, min_periods=1).min()\n","    high_max = data['High'].rolling(window=periods, min_periods=1).max()\n","\n","    k = 100 * ((data['Close'] - low_min) / (high_max - low_min).replace(0, np.nan))\n","    data['Stochastic_%K'] = k\n","    data['Stochastic_%D'] = k.rolling(window=3, min_periods=1).mean()  # Moving average of %K\n","    return data\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def add_historical_features(data):\n","    # Rolling Average Volume (20 days)\n","    data['Rolling_Avg_Vol_20'] = data['Volume'].rolling(window=20).mean()\n","    \n","    # Volatility\n","    data['Volatility_20'] = data['Close'].pct_change().rolling(window=20).std()\n","    \n","    # Relative Price Oscillator (RPO)\n","    short_ema = data['Close'].ewm(span=12, adjust=False).mean()\n","    long_ema = data['Close'].ewm(span=26, adjust=False).mean()\n","    data['RPO'] = short_ema - long_ema\n","    \n","    # Cumulative Returns\n","    data['Cum_Return_1M'] = data['Close'].pct_change(20).cumsum()  # 20 trading days in a month\n","    data['Cum_Return_2M'] = data['Close'].pct_change(40).cumsum()  # 40 trading days in two months\n","    \n","\n","    # Initialize columns\n","    data['Days_since_High'] = 0\n","    data['Days_since_Low'] = 0\n","\n","    # Tracks the last occurrence index\n","    last_high = last_low = None\n","\n","    # Iterate over DataFrame rows\n","    for index, row in data.iterrows():\n","        if last_high is None or row['Close'] >= data.loc[last_high, 'Close']:\n","            last_high = index\n","        if last_low is None or row['Close'] <= data.loc[last_low, 'Close']:\n","            last_low = index\n","        \n","        # Calculate days since last high and low\n","        data.at[index, 'Days_since_High'] = index - last_high\n","        data.at[index, 'Days_since_Low'] = index - last_low\n","\n","    \n","    # Exponential Decay Moving Average\n","    data['EDMA'] = data['Close'].ewm(span=20, adjust=False).mean()\n","    \n","    # VWAP\n","    data['VWAP'] = (data['Volume'] * (data['High'] + data['Low'] + data['Close']) / 3).cumsum() / data['Volume'].cumsum()\n","    \n","    # Price Count\n","    data['Price_Up_Count'] = data['Close'].rolling(window=20).apply(lambda x: (np.diff(x) > 0).sum())\n","    data['Price_Down_Count'] = data['Close'].rolling(window=20).apply(lambda x: (np.diff(x) < 0).sum())\n","    \n","    # Gap Detection\n","    data['Gap'] = data['Open'] - data['Close'].shift(1)\n","    \n","    # Global Stock Rate Change (GSRC)\n","\n","    # Calculate the total 'Close' for each day\n","    daily_totals = data.groupby('Date')['Close'].sum()\n","\n","    # Calculate the day-to-day percentage change in total 'Close' values\n","    daily_change = daily_totals.pct_change().fillna(0)  # Replace NaN with 0 for the first day\n","\n","    # Map these changes back to the original DataFrame based on the 'Date'\n","    data['Global_SRC'] = data['Date'].map(daily_change)\n","    return data\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["def add_features(data):\n","    data = calculate_obv(data)\n","    data = calculate_ad_line(data)\n","    data = calculate_adx(data)\n","    data = calculate_aroon(data)\n","    data = calculate_macd(data)\n","    data = calculate_rsi(data)\n","    data = calculate_stochastic_oscillator(data)\n","\n","    data = add_historical_features(data)\n","\n","    return data"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["training models\n","                          Fold 1    Fold 2    Fold 3    Fold 4    Fold 5  \\\n","GaussianNB              0.851119  0.876911  0.877349  0.877257  0.877349   \n","LogisticRegression      0.853553  0.877339  0.877339  0.877345  0.877345   \n","RandomForestClassifier  0.862737  0.848261  0.807888  0.820517  0.877366   \n","ExtraTreesClassifier    0.872087  0.863588  0.837930  0.835468  0.877354   \n","XGBClassifier           0.702003  0.849078  0.729215  0.688516  0.877358   \n","\n","                         Average  \n","GaussianNB              0.871997  \n","LogisticRegression      0.872584  \n","RandomForestClassifier  0.843354  \n","ExtraTreesClassifier    0.857285  \n","XGBClassifier           0.769234  \n"]}],"source":["data_train = pd.read_csv(\"train_clean.csv\")\n","data_train = data_train.sort_values(by=['Symbol', 'Date'], ascending=[True, True])\n","data_test = pd.read_csv(\"test_clean.csv\")\n","data_test = data_test.sort_values(by=['Symbol', 'Date'], ascending=[True, True])\n","\n","features = ['Close', 'High', 'Low', 'Open', 'Volume', 'OBV', 'AD Line', 'ADX', 'Aroon Up', 'Aroon Down', 'MACD', 'RSI', 'Stochastic_%K', 'Stochastic_%D', 'Rolling_Avg_Vol_20', 'Volatility_20', 'RPO', 'Cum_Return_1M', 'Cum_Return_2M', 'EDMA', 'VWAP', 'Price_Up_Count', 'Price_Down_Count', 'Gap', 'Global_SRC']\n","data_train = add_features(data_train)\n","data_train = data_train.fillna(0)\n","data_test = add_features(data_test)\n","data_test = data_test.fillna(0)\n","\n","print(\"training models\")\n","cross_validate_models(data_train, data_test, features, 2)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["data_train = pd.read_csv(\"train_clean.csv\")\n","data_train = data_train.sort_values(by=['Symbol', 'Date'], ascending=[True, True])\n","data_test = pd.read_csv(\"test_clean.csv\")\n","data_test = data_test.sort_values(by=['Symbol', 'Date'], ascending=[True, True])\n","\n","features = ['Close', 'High', 'Low', 'Open', 'Volume', 'OBV', 'AD Line', 'ADX', 'Aroon Up', 'Aroon Down', 'MACD', 'RSI', 'Stochastic_%K', 'Stochastic_%D', 'Rolling_Avg_Vol_20', 'Volatility_20', 'RPO', 'Cum_Return_1M', 'Cum_Return_2M', 'EDMA', 'VWAP', 'Price_Up_Count', 'Price_Down_Count', 'Gap', 'Global_SRC']\n","data_train = add_features(data_train)\n","data_train = data_train.fillna(0)\n","data_test = add_features(data_test)\n","data_test = data_test.fillna(0)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["def feature_selection(train, initial_features):\n","\n","    X = train[initial_features]\n","    y = train['Target']\n","\n","    current_features = initial_features[:]\n","    while len(current_features) > 12:\n","        scores = []\n","        for feature in current_features:\n","            temp_features = [f for f in current_features if f != feature]\n","            X_train, X_test, y_train, y_test = train_test_split(X[temp_features], y, test_size=0.25, random_state=42)\n","            model = LinearSVC(dual=False, max_iter=5000)\n","            model.fit(X_train, y_train)\n","            y_pred = model.predict(X_test[temp_features])\n","            scores.append(f1_score(y_test, y_pred))\n","\n","        # Determine which feature to remove\n","        best_score_index = np.argmax(scores)\n","        feature_to_remove = current_features[best_score_index]\n","        current_features.remove(feature_to_remove)\n","        print(f\"Removed {feature_to_remove}, remaining features: {len(current_features)}\")\n","    \n","    return current_features"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Removed OBV, remaining features: 15\n","Removed AD Line, remaining features: 14\n","Removed ADX, remaining features: 13\n","Removed Aroon Up, remaining features: 12\n","Selected features: ['Aroon Down', 'MACD', 'RSI', 'Stochastic_%K', 'Stochastic_%D', 'Rolling_Avg_Vol_20', 'RPO', 'EDMA', 'VWAP', 'Price_Up_Count', 'Price_Down_Count', 'Global_SRC']\n"]}],"source":["def wrapper(data_train):\n","    features = ['OBV', 'AD Line', 'ADX', 'Aroon Up', 'Aroon Down', 'MACD', 'RSI', \n","                'Stochastic_%K', 'Stochastic_%D', 'Rolling_Avg_Vol_20', 'RPO', 'EDMA', \n","                'VWAP', 'Price_Up_Count', 'Price_Down_Count', 'Global_SRC']\n","\n","    # Perform feature selection\n","    selected_features = feature_selection(data_train,  features)\n","    print(\"Selected features:\", selected_features)\n","\n","    return selected_features\n","\n","\n","selected_features = wrapper(data_train)"]},{"cell_type":"code","execution_count":73,"metadata":{},"outputs":[],"source":["cross_validate_models(data_train, data_test, selected_features, 3)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def plot_stock_trends(stock_data, symbol, features, ax=None):\n","    stock_specific_data = stock_data.loc[stock_data['Symbol'] == symbol].copy()\n","    stock_specific_data['Date'] = pd.to_datetime(stock_specific_data['Date'])\n","    \n","    # Create a new figure and axis object if not provided\n","    if ax is None:\n","        fig, ax = plt.subplots(figsize=(14, 7))\n","    \n","    # Plot each feature on the y-axis\n","    for feature in features:\n","        ax.plot(stock_specific_data['Date'], stock_specific_data[feature], label=feature)\n","    \n","    # Formatting the plot\n","    ax.set_title(f'Stock Trends for {symbol}')\n","    ax.set_xlabel('Date')\n","    ax.set_ylabel('Value')\n","    ax.legend()\n","    \n","    return ax"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[106, 107, 108, 109, 110, 107, 108, 109, 110, 111, 108, 109, 110, 111, 112, 119, 120, 121, 122, 123, 120, 121, 122, 123, 124, 121, 122, 123, 124, 125, 122, 123, 124, 125, 126, 123, 124, 125, 126, 127, 301, 302, 303, 304, 305, 302, 303, 304, 305, 306, 303, 304, 305, 306, 307, 304, 305, 306, 307, 308, 315, 316, 317, 318, 319, 316, 317, 318, 319, 320, 317, 318, 319, 320, 321, 318, 319, 320, 321, 322, 319, 320, 321, 322, 323, 440, 441, 442, 443, 444, 441, 442, 443, 444, 445, 442, 443, 444, 445, 446, 443, 444, 445, 446, 447, 444, 445, 446, 447, 448, 453, 454, 455, 456, 457, 454, 455, 456, 457, 458, 455, 456, 457, 458, 459, 456, 457, 458, 459, 460, 457, 458, 459, 460, 461, 584, 585, 586, 587, 588, 585, 586, 587, 588, 589, 586, 587, 588, 589, 590, 587, 588, 589, 590, 591, 588, 589, 590, 591, 592, 787, 788, 789, 790, 791, 788, 789, 790, 791, 792, 789, 790, 791, 792, 793, 790, 791, 792, 793, 794, 791, 792, 793, 794, 795, 813, 814, 815, 816, 817, 814, 815, 816, 817, 818, 815, 816, 817, 818, 819, 816, 817, 818, 819, 820, 817, 818, 819, 820, 821, 843, 844, 845, 846, 847, 844, 845, 846, 847, 848, 845, 846, 847, 848, 849, 846, 847, 848, 849, 850, 847, 848, 849, 850, 851, 1023, 1024, 1025, 1026, 1027, 1024, 1025, 1026, 1027, 1028, 1025, 1026, 1027, 1028, 1029, 1026, 1027, 1028, 1029, 1030, 1027, 1028, 1029, 1030, 1031, 1294, 1295, 1296, 1297, 1298, 1295, 1296, 1297, 1298, 1299, 1296, 1297, 1298, 1299, 1300, 1297, 1298, 1299, 1300, 1301, 1298, 1299, 1300, 1301, 1302, 1333, 1334, 1335, 1336, 1337, 1334, 1335, 1336, 1337, 1338, 1335, 1336, 1337, 1338, 1339, 1336, 1337, 1338, 1339, 1340, 1337, 1338, 1339, 1340, 1341, 1355, 1356, 1357, 1358, 1359, 1356, 1357, 1358, 1359, 1360, 1357, 1358, 1359, 1360, 1361, 1358, 1359, 1360, 1361, 1362, 1359, 1360, 1361, 1362, 1363, 1397, 1398, 1399, 1400, 1401, 1398, 1399, 1400, 1401, 1402, 1399, 1400, 1401, 1402, 1403, 1400, 1401, 1402, 1403, 1404, 1401, 1402, 1403, 1404, 1405, 1500, 1501, 1502, 1503, 1504, 1501, 1502, 1503, 1504, 1505, 1502, 1503, 1504, 1505, 1506, 1503, 1504, 1505, 1506, 1507, 1504, 1505, 1506, 1507, 1508, 1580, 1581, 1582, 1583, 1584, 1581, 1582, 1583, 1584, 1585, 1582, 1583, 1584, 1585, 1586, 1583, 1584, 1585, 1586, 1587, 1584, 1585, 1586, 1587, 1588, 1681, 1682, 1683, 1684, 1685, 1682, 1683, 1684, 1685, 1686, 1683, 1684, 1685, 1686, 1687, 1684, 1685, 1686, 1687, 1688, 1685, 1686, 1687, 1688, 1689, 1702, 1703, 1704, 1705, 1706, 1703, 1704, 1705, 1706, 1707, 1704, 1705, 1706, 1707, 1708, 1705, 1706, 1707, 1708, 1709, 1706, 1707, 1708, 1709, 1710, 2016, 2017, 2018, 2019, 2020, 2017, 2018, 2019, 2020, 2021, 2018, 2019, 2020, 2021, 2022, 2019, 2020, 2021, 2022, 2023, 2020, 2021, 2022, 2023, 2024, 2240, 2241, 2242, 2243, 2244, 2241, 2242, 2243, 2244, 2245, 2242, 2243, 2244, 2245, 2246, 2243, 2244, 2245, 2246, 2247, 2244, 2245, 2246, 2247, 2248, 2340, 2341, 2342, 2343, 2344, 2341, 2342, 2343, 2344, 2345, 2342, 2343, 2344, 2345, 2346, 2343, 2344, 2345, 2346, 2347, 2344, 2345, 2346, 2347, 2348]\n"]}],"source":["def identify_special_indices(data):\n","\n","    indices = []\n","    for i in range(len(data)):\n","        if i < 5:  # Ensure there's enough data before the current index\n","            continue\n","        # Check if the current and next four entries have a target of 1\n","        if data['Target'].iloc[i:i+5].sum() == 5:\n","            # Check if there's at least one zero in the five days before the current day\n","            if (data['Target'].iloc[i-5:i] == 0).any():\n","                indices.extend(range(i, i+5))  # Add indices of the 5 consecutive days to the list\n","\n","    return indices\n","\n","# Example usage with your data:\n","selected_indices = identify_special_indices(data_train[data_train['Symbol'] == 'AAPL'])\n","print(selected_indices)\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import shap\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Train the model\n","model = RandomForestClassifier(random_state=420)\n","model.fit(data_train[selected_features], data_train['Target'])\n","\n","# Create SHAP explainer and values\n","explainer = shap.TreeExplainer(model)\n","shap_values = explainer.shap_values(data_train[selected_features])\n","\n","# Data isolation logic\n","indices = identify_special_indices(data_train)\n","\n","# SHAP visualization\n","for i in indices:\n","    shap.force_plot(explainer.expected_value[1], shap_values[1][i,:], data_train.iloc[i,:])\n","\n","# Plotting\n","plot_stock_trends(data_train, 'AAPL', ['Close', 'Volume', 'ADX'])\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["F1 Score: 0.9141965678627145\n","[[VWAP=15.1-15.88] V [Cum_Return_2M=109.62-116.99] V [Id=44369.2-44621.4] V [EDMA=<12.14] V [VWAP=>19.64^TR14=>1.06] V [EDMA=15.47-18.89^VWAP=14.7-15.1^ADLine=13693026082.55-15561189180.52] V [ADLine=15561189180.52-16577130747.53^-DM=<0.082^Open=15.14-18.61] V [Id=44116.4-44369.2^OBV=7233400160.0-8976385520.0] V [EDMA=12.14-15.47^VWAP=11.12-13.06] V [TR14=0.75-1.06^OBV=12071871760.0-12482838640.0] V [Rolling_Avg_Vol_20=227451240.0-298296040.0^AdjClose=25.66-29.11] V [VWAP=16.98-17.49^Cum_Return_1M=45.62-48.03^OBV=8976385520.0-9993894400.0] V [Rolling_Avg_Vol_20=382010580.0-479008964.0^Signal_Line=-0.05-0.05] V [Rolling_Avg_Vol_20=<103363488.0^+DM14=>0.2] V [OBV=>14534347600.0^High=18.8-21.77] V [AdjClose=22.45-25.66^ADX=>50.68] V [AdjClose=22.45-25.66^ADX=39.89-50.68] V [MACD=-0.056-0.047^AroonDown=56.0-72.0] V [Rolling_Avg_Vol_20=382010580.0-479008964.0^Volatility_20=0.014-0.016] V [-DM14=0.078-0.089^Close=15.17-18.61]]\n"]}],"source":["import wittgenstein as lw\n","stock_data = data_train[data_train['Symbol'] == 'AAPL']\n","\n","# Split the data\n","train, test = train_test_split(stock_data, test_size=0.33, random_state=420)\n","\n","ripper = lw.RIPPER(random_state=42)\n","ripper.fit(train.drop('Target', axis=1), train['Target'])\n","\n","# Predict and calculate F1 score\n","preds = ripper.predict(test.drop('Target', axis=1))\n","print(f\"F1 Score: {f1_score(test['Target'], preds)}\")\n","\n","# Print the rules\n","print(ripper.ruleset_)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Rules:\n","[VWAP=15.1-15.88]\n","[Cum_Return_2M=109.62-116.99]\n","[Id=44369.2-44621.4]\n","[EDMA=<12.14]\n","[VWAP=>19.64^TR14=>1.06]\n","[EDMA=15.47-18.89^VWAP=14.7-15.1^ADLine=13693026082.55-15561189180.52]\n","[ADLine=15561189180.52-16577130747.53^-DM=<0.082^Open=15.14-18.61]\n","[Id=44116.4-44369.2^OBV=7233400160.0-8976385520.0]\n","[EDMA=12.14-15.47^VWAP=11.12-13.06]\n","[TR14=0.75-1.06^OBV=12071871760.0-12482838640.0]\n","[Rolling_Avg_Vol_20=227451240.0-298296040.0^AdjClose=25.66-29.11]\n","[VWAP=16.98-17.49^Cum_Return_1M=45.62-48.03^OBV=8976385520.0-9993894400.0]\n","[Rolling_Avg_Vol_20=382010580.0-479008964.0^Signal_Line=-0.05-0.05]\n","[Rolling_Avg_Vol_20=<103363488.0^+DM14=>0.2]\n","[OBV=>14534347600.0^High=18.8-21.77]\n","[AdjClose=22.45-25.66^ADX=>50.68]\n","[AdjClose=22.45-25.66^ADX=39.89-50.68]\n","[MACD=-0.056-0.047^AroonDown=56.0-72.0]\n","[Rolling_Avg_Vol_20=382010580.0-479008964.0^Volatility_20=0.014-0.016]\n","[-DM14=0.078-0.089^Close=15.17-18.61]\n"]}],"source":["print(\"Rules:\")\n","for rule in ripper.ruleset_:\n","    print(rule)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Best Score: 0.9333838001514005\n","Best Params: {'k': 3, 'prune_size': 0.33, 'dl_allowance': 0.5}\n","[[Id=44369.2-44621.4] V [VWAP=15.1-15.88] V [Cum_Return_2M=109.62-116.99]]\n"]}],"source":["param_grid = {\n","    'k': [1, 2, 3, 5],\n","    'prune_size': [0.1, 0.2, 0.33],\n","    'dl_allowance': [0.1, 0.5, 1]\n","}\n","\n","# Wrapper to use RIPPER with GridSearchCV\n","def RIPPER_GridSearch(X, y, param_grid, cv=3):\n","    best_score = 0\n","    best_params = {}\n","    for k in param_grid['k']:\n","        for prune_size in param_grid['prune_size']:\n","            for dl_allowance in param_grid['dl_allowance']:\n","                model = lw.RIPPER(k=k, prune_size=prune_size, dl_allowance=dl_allowance)\n","                model.fit(X, y)\n","                score = f1_score(y, model.predict(X))\n","                if score > best_score:\n","                    best_score = score\n","                    best_params = {'k': k, 'prune_size': prune_size, 'dl_allowance': dl_allowance}\n","    return best_score, best_params\n","\n","best_score, best_params = RIPPER_GridSearch(train.drop('Target', axis=1), train['Target'], param_grid)\n","print(\"Best Score:\", best_score)\n","print(\"Best Params:\", best_params)\n","\n","\n","ripper = lw.RIPPER(max_rules=3, max_rule_conds=2, **best_params)\n","ripper.fit(train.drop('Target', axis=1), train['Target'])\n","print(ripper.ruleset_)\n"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8057312,"sourceId":73206,"sourceType":"competition"}],"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":4}
